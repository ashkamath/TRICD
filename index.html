---
layout: default
---

<!DOCTYPE html>
<html>

    <head lang="en">
        <meta charset="UTF-8">
        <meta http-equiv="x-ua-compatible" content="ie=edge">
        <meta name="description" content="TRICD - A new human labelled dataset for testing robust image understanding.">
        <meta name="viewport" content="width=device-width, initial-scale=1">

        <!-- <base href="/"> -->

        <!--FACEBOOK-->
        <!--<meta property="og:image" content="https://ashkamath.github.io/TRICD/assets/tricd_main_image.png">
        <meta property="og:image:type" content="image/png">
        <meta property="og:image:width" content="512">
        <meta property="og:image:height" content="1024">
        <meta property="og:type" content="website" />
        <meta property="og:url" content="https://ashkamath.github.io/TRICD/"/>
        <meta property="og:title" content="TRICD: Testing Robust Image Understanding Through Contextual Phrase Detection" />
        <meta property="og:description" content="TRICD is a new human labelled dataset for testing robust image understanding. It enables evaluation on a novel task that we propose - Contextual Phrase Detection." />-->

        <!--TWITTER-->
        <!--<meta name="twitter:card" content="summary_large_image" />
        <meta name="twitter:title" content="TRICD: Testing Robust Image Understanding Through Contextual Phrase Detection" />
        <meta name="twitter:description" content="TRICD - A new human labelled dataset for testing robust image understanding." />
        <meta name="twitter:image" content="https://ashkamath.github.io/TRICD/assets/tricd_main_image.png" />-->



        <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.0/css/bootstrap.min.css">
        <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css">

        <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.min.js"></script>

    </head>
<body>
        <div class="container" id="main">
            <div class="row">
                <div class="col-md-12 text-center">
                    <ul class="list-inline">
                        <li class="list-inline-item">
                            <a href="https://ashkamath.github.io/">
                                Aishwarya Kamath* &dagger;
                            </a>
                        </li>
                        <li class="list-inline-item">
                            <a href="https://github.com/sbp354">
                                Sara Price*  &dagger;
                            </a>
                        </li>
			<li class="list-inline-item">
                            <a href="https://pfeiffer.ai/">
                                Jonas Pfeiffer  &dagger;
                            </a>
                        </li>
                        <li class="list-inline-item">
                            <a href="https://en.wikipedia.org/wiki/Yann_LeCun">
                                Yann LeCun  &dagger;&Dagger;
                            </a>
                        </li>
                        <li class="list-inline-item">
                            <a href="https://scholar.google.com/citations?user=h8u3ll8AAAAJ&hl=fr">
                                Nicolas Carion*  &dagger;&Dagger;
                            </a>
                        </li>
                    </ul>
                    * equal contribution   &dagger;New York University    &Dagger;Meta AI
                </div>
            </div>
            <div class="row">
                &nbsp
            </div>


            <div class="row">
                <div class="col-sm-12 text-center">
                    <ul class="nav justify-content-center nav-fill">
                        <li class="nav-item">
                            <a href="https://arxiv.org/abs/">
                                <image src="assets/paper.png" height="60px">
                                    <h4><strong>Paper</strong></h4>
                            </a>
                        </li>
                        <li class="nav-item">
                            <a href="https://github.com/sbp354/TRICD">
                                <image src="assets/github.png" height="60px">
                                    <h4><strong>Code</strong></h4>
                            </a>
                        </li>
			<li class="nav-item">
                            <a href="https://github.com/ashkamath/TRICD">
                                <image src="assets/evalai-logo-single.png" height="60px">
                                    <h4><strong>Evaluation Server</strong></h4>
                            </a>
                        </li>
			<li class="nav-item">
                            <a href="https://github.com/ashkamath/TRICD">
                                <image src="assets/evalai-logo-single.png" height="60px">
                                    <h4><strong>Leaderboard</strong></h4>
                            </a>
                        </li>
                        <!-- <li class="nav-item">
                            <a href="https://colab.research.google.com/github/ashkamath/mdetr/blob/colab/notebooks/MDETR_demo.ipynb">
                                <image src="assets/colab.png" height="60px">
                                    <h4><strong>Colab</strong></h4>
                            </a>
                        </li> -->
                    </ul>
                </div>
            </div>
	 <div class="row justify-content-center">
                <div class="col-xl-12">
		     <center>
                     <image src="assets/teaser1.png" class="img-fluid" alt="Teaser figure"><br>
                      Current state of the art VQA systems while displaying impressive performance on popular benchmarks, often fail when evaluated on challenging images such as those shown above. We propose a novel task titled "Contextual Phrase Detection" which evaluates models' fine-grained vision and language understanding capabilities along with a human-labelled dataset (TRICD) that enables evaluation on this novel task. 
			
			 <div class="col-xl-12">
                    <h2>FAQs</h2>
			<p class="text-justify">
                    <ul>
                     <li> <b>What is Contextual Phrase Detection (CPD)? : </b> A single task that subsumes object detection, phrase grounding and visual question answering, in which the model must use full context from the given sentence, decide whether this sentence holds true for the given image, and if so, return bounding boxes for all relevant objects referred to in the sentence.</li>
                     <li> <b>How is the TRICD dataset different from other benchmarks? : </b> We manually curate image-text pairs with bounding boxes for each of the phrases present in the image. The pairs are contextually related, but partially contradictory; i.e. while the images and texts are semantically similar, each sentence is only depicted in one of the images, but not the other. We manually verify and ensure the negatives.  
		     <li> <b>But is it really that much harder than existing classification and grounding benchmarks? : </b> Yes! We show that by decomposing our task into classification and grounding, we can compare model's performance on sub-splits of GQA and Flickr30k, and find that our benchmark has a significant gap over these prior benchmarks.</li>
        	    <li> <b> What is the metric that is used?: We compute Average Precision as is common in object detection literature.</b>
		    <li> <b>Where do the images come from? </b> We use two data distributions - COCO and Winoground as our pool of images from which we build our dataset.</li>                
		</ul></b>
			</p>
            </div>



 <image src="assets/annotation_process.png" class="img-fluid" alt="Annotation process"><br>

			<h2>
                            Abstract
                        </h2>
                    </center>
                        <p class="text-justify">
                        Most traditional benchmarks for computer vision focus on tasks that use a fixed set of labels that are known a priori. On the other hand, tasks like phrase grounding and referring expression comprehension make it possible to probe the model through natural language, which allows us to gain a more extensive understanding of the model's visual understanding capabilities. However, unlike object detection, these free-form text-conditioned box prediction tasks all operate under the assumption that the text corresponds to objects that are necessarily present in the image. We show that results on such benchmarks tend to overestimate the capabilities of models significantly given that models do not necessarily need to understand the context, but merely localize the named entities. In this work we aim to highlight this blind spot in model evaluation by proposing a novel task: 
   Contextual Phrase Detection (CPD). To evaluate it, we release a human annotated evaluation dataset called TRICD (<b>T</b>esting <b>R</b>obust <b>I</b>mage understanding through <b>C</b>ontextual Phrase <b>D</b>etection, pronounced "tricked"). It consists of instances of two image-text pairs with bounding boxes for each of the phrases present in the image. The pairs are contextually related, but partially contradictory; i.e. while the images and texts are semantically similar, each sentence is only depicted in one of the images, but not the other. Models must predict the relevant bounding boxes for the phrases in an image if and only if it is in accordance with the context defined by the full sentence. We benchmark the performance of several state of the art multi-modal models on this task in terms of average precision (AP).
			</p>
                </div>
            </div>
	</body>
</html>
