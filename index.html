---
layout: default
---

<!DOCTYPE html>
<html>

    <head lang="en">
        <meta charset="UTF-8">
        <meta http-equiv="x-ua-compatible" content="ie=edge">
        <meta name="description" content="TRICD - A new human labelled dataset for testing robust image understanding.">
        <meta name="viewport" content="width=device-width, initial-scale=1">

        <!-- <base href="/"> -->

        <!--FACEBOOK-->
        <!--<meta property="og:image" content="https://ashkamath.github.io/TRICD/assets/tricd_main_image.png">
        <meta property="og:image:type" content="image/png">
        <meta property="og:image:width" content="512">
        <meta property="og:image:height" content="1024">
        <meta property="og:type" content="website" />
        <meta property="og:url" content="https://ashkamath.github.io/TRICD/"/>
        <meta property="og:title" content="TRICD: Testing Robust Image Understanding Through Contextual Phrase Detection" />
        <meta property="og:description" content="TRICD is a new human labelled dataset for testing robust image understanding. It enables evaluation on a novel task that we propose - Contextual Phrase Detection." />-->

        <!--TWITTER-->
        <!--<meta name="twitter:card" content="summary_large_image" />
        <meta name="twitter:title" content="TRICD: Testing Robust Image Understanding Through Contextual Phrase Detection" />
        <meta name="twitter:description" content="TRICD - A new human labelled dataset for testing robust image understanding." />
        <meta name="twitter:image" content="https://ashkamath.github.io/TRICD/assets/tricd_main_image.png" />-->



        <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.0/css/bootstrap.min.css">
        <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css">

        <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.min.js"></script>

    </head>
<body>
        <div class="container" id="main">
            <div class="row">
                <div class="col-md-12 text-center">
                    <ul class="list-inline">
                        <li class="list-inline-item">
                            <a href="https://ashkamath.github.io/">
                                Aishwarya Kamath* &dagger;
                            </a>
                        </li>
                        <li class="list-inline-item">
                            <a href="https://github.com/sbp354">
                                Sara Price*  &dagger;
                            </a>
                        </li>
			<li class="list-inline-item">
                            <a href="https://pfeiffer.ai/">
                                Jonas Pfeiffer  &dagger;
                            </a>
                        </li>
                        <li class="list-inline-item">
                            <a href="https://en.wikipedia.org/wiki/Yann_LeCun">
                                Yann LeCun  &dagger;&Dagger;
                            </a>
                        </li>
                        <li class="list-inline-item">
                            <a href="https://scholar.google.com/citations?user=h8u3ll8AAAAJ&hl=fr">
                                Nicolas Carion  &dagger;&Dagger;
                            </a>
                        </li>
                    </ul>
                    * equal contribution   &dagger;New York University    &Dagger;Meta AI
                </div>
            </div>
            <div class="row">
                &nbsp
            </div>


            <div class="row">
                <div class="col-sm-12 text-center">
                    <ul class="nav justify-content-center nav-fill">
                        <li class="nav-item">
                            <a href="https://arxiv.org/abs/">
                                <image src="assets/paper.png" height="60px">
                                    <h4><strong>Paper</strong></h4>
                            </a>
                        </li>
                        <li class="nav-item">
                            <a href="https://github.com/sbp354/TRICD">
                                <image src="assets/github.png" height="60px">
                                    <h4><strong>Code</strong></h4>
                            </a>
                        </li>
			<li class="nav-item">
                            <a href="https://github.com/ashkamath/TRICD">
                                <image src="assets/evalai-logo-single.png" height="60px">
                                    <h4><strong>Evaluation Server</strong></h4>
                            </a>
                        </li>
			<li class="nav-item">
                            <a href="https://github.com/ashkamath/TRICD">
                                <image src="assets/evalai-logo-single.png" height="60px">
                                    <h4><strong>Leaderboard</strong></h4>
                            </a>
                        </li>
                        <!-- <li class="nav-item">
                            <a href="https://colab.research.google.com/github/ashkamath/mdetr/blob/colab/notebooks/MDETR_demo.ipynb">
                                <image src="assets/colab.png" height="60px">
                                    <h4><strong>Colab</strong></h4>
                            </a>
                        </li> -->
                    </ul>
                </div>
            </div>
	 <div class="row justify-content-center">
                <div class="col-xl-12">
                     <image src="assets/tricd_main_image.png" class="img-fluid" alt="Model"><br> -->
                    <center>
                        <h2>
                            Abstract
                        </h2>
                    </center>
                        <p class="text-justify">
                        Most traditional benchmarks for computer vision focus on tasks that use a fixed set of labels that are known a priori. On the other hand, tasks like phrase grounding and referring expression comprehension make it possible to probe the model through natural language, which allows us to gain a more extensive understanding of the model's visual understanding capabilities. However, unlike object detection, these free-form text-conditioned box prediction tasks all operate under the assumption that the text corresponds to objects that are necessarily present in the image. We show that results on such benchmarks tend to overestimate the capabilities of models significantly given that models do not necessarily need to understand the context, but merely localize the named entities. In this work we aim to highlight this blind spot in model evaluation by proposing a novel task: 
   Contextual Phrase Detection (CPD). To evaluate it, we release a human annotated evaluation dataset called TRICD (<b>T</b>esting <b>R</b>obust <b>I</b>mage understanding through <b>C</b>ontextual Phrase <b>D</b>etection, pronounced "tricked"). It consists of instances of two image-text pairs with bounding boxes for each of the phrases present in the image. The pairs are contextually related, but partially contradictory; i.e. while the images and texts are semantically similar, each sentence is only depicted in one of the images, but not the other. Models must predict the relevant bounding boxes for the phrases in an image if and only if it is in accordance with the context defined by the full sentence. We benchmark the performance of several state of the art multi-modal models on this task in terms of average precision (AP).
			</p>
                </div>
            </div>
	</body>
</html>
